{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Granger_Intraday1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9zjtAtCGyG6I","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3wGyuzrgLo1","colab_type":"code","colab":{}},"source":["!pip install alpha_vantage\n","!pip install vaderSentiment\n","!pip install googletrans\n","!pip install ratelimit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqL0RIF7gTvG","colab_type":"code","colab":{}},"source":["import tweepy\n","from tweepy import OAuthHandler\n","import datetime\n","import json\n","import csv\n","from pprint import pprint\n","import pandas as pd\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n","import re\n","import string\n","import seaborn as sns\n","import numpy as np\n","from alpha_vantage.timeseries import TimeSeries\n","import matplotlib.pyplot as plt\n","from googletrans import Translator\n","translator = Translator()\n","from google.colab import drive\n","import statsmodels.tsa.stattools as ts\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cv-LWbEBgyMx","colab_type":"code","colab":{}},"source":["#ALPHA-VANTAGE INTRADAY STOCK DATA: 1MIN INTERVAL\n","ts1 = TimeSeries(key='2ZJEXZ6IRC95SZHM', output_format='pandas')\n","stocks_1=[]\n","stocks_2=[]\n","min_pvalue=[]\n","best_lag=[]\n","for stock1 in stock_list:\n","  print(stock1)\n","  try:\n","    data1, meta_data1 = ts1.get_intraday(symbol=stock1,interval='5min', outputsize='full')\n","  except ValueError:\n","    sleep_and_retry\n","    print('1')\n","  for stock2 in stock_list:\n","    if(stock1==stock2):\n","      continue\n","    try:\n","      data2, meta_data2 = ts1.get_intraday(symbol=stock2,interval='5min', outputsize='full')\n","    except ValueError:\n","      sleep_and_retry\n","      print('2')\n","    stocks_1.append(stock1)\n","    stocks_2.append(stock2)\n","    x1=data1['4. close'].tolist()\n","    y1=data2['4. close'].tolist()\n","    data=list(zip(x1,y1))\n","    granger_result=ts.grangercausalitytests(data, maxlag=50, verbose=False)\n","\n","    try:\n","      pvalues=[]\n","      for i in range (50):\n","        ad=i+1\n","        a1=granger_result[ad]\n","        a1_=a1[0]\n","        x_params_1=a1_['params_ftest']\n","        x_likelihhod_ratio_1=a1_['lrtest']\n","        x_SSR_chi2_1=a1_['ssr_chi2test']\n","        pvalue=((x_params_1[1]+x_likelihhod_ratio_1[1]+x_SSR_chi2_1[1])/3) \n","        pvalues.append(pvalue)\n","      min_pval=min(pvalues)\n","      min_index=index(min(pvalues))+1\n","      min_pvalue.append(min_pval)\n","      best_lag.append(min_index)\n","    except:\n","      min_pvalue.append(1.0)\n","      best_lag.append(0.0)\n","\n","dict34={'Stock1':stocks_1 , 'Stock2':stocks_2 , 'Min. pval':min_pvalue , 'Best lag':best_lag}\n","intraday_stock_granger=pd.DataFrame(data=dict34)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Td_Rt8_6F_9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"37468bfd-40f8-4507-a96c-2e0f8d84b9ac"},"source":["stock_list=['MSFT','AAPL','AMZN','GOOGL','ORCL','JPM','GS','AXP','HSBC','V','BAC','T','JNJ','NVS','MRK','UNH','XOM','CVX','TOT','TM','F','NSANY','TSLA','GM','HOG','BA','DAL','LUV','UAL','AAL','SAVE','MCD','CMG','KMX','MUSA','CTB','SAH','GPC','H','LVS','MAR','PFE','PG','CHL','ABT','MDT','WMT','KO','PEP','BABA','NKE','DIS','NFLX','EROS','WWE','CNK','DELL','IBM','HPQ','FIT','HMI','TSM','USEG','PTR','SNP','UNP','SPCE','LMT','NOC','UPS','FDX','RIO','VZ','SAP','UL']\n","#stock_list=bad_tickers\n","ts1 = TimeSeries(key='2ZJEXZ6IRC95SZHM', output_format='pandas')\n","#bad_tickers=[]\n","df_mas=pd.DataFrame()\n","for stock in bad_tickers:\n","  try:\n","    data, meta_data = ts1.get_intraday(symbol=stock1,interval='5min', outputsize='full')\n","    df_mas['stock '+stock]=data['4. close'].tolist()\n","    time.sleep(10)\n","  except Exception as e:\n","    bad_tickers.append(stock)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MSFT1\n","MSFT\n","AAPL1\n","AAPL\n","AMZN1\n","AMZN\n","GOOGL1\n","GOOGL\n","ORCL1\n","ORCL\n","JPM1\n","JPM\n","BRK1\n","BRK\n","GS1\n","GS\n","AXP1\n","AXP\n","HSBC1\n","HSBC\n","V1\n","V\n","BAC1\n","BAC\n","T1\n","T\n","JNJ1\n","JNJ\n","NVS1\n","NVS\n","MRK1\n","MRK\n","UNH1\n","UNH\n","XOM1\n","XOM\n","CVX1\n","CVX\n","TOT1\n","TOT\n","TM1\n","TM\n","F1\n","F\n","NSANY1\n","NSANY\n","TSLA1\n","TSLA\n","GM1\n","GM\n","HOG1\n","HOG\n","BA1\n","BA\n","DAL1\n","LUV1\n","UAL1\n","AAL1\n","SAVE1\n","MCD1\n","CMG1\n","KMX1\n","KMX\n","MUSA1\n","MUSA\n","CTB1\n","CTB\n","SAH1\n","SAH\n","GPC1\n","GPC\n","H1\n","H\n","LVS1\n","LVS\n","MAR1\n","MAR\n","PFE1\n","PFE\n","PG1\n","PG\n","CHL1\n","CHL\n","ABT1\n","ABT\n","MDT1\n","MDT\n","WMT1\n","WMT\n","KO1\n","KO\n","PEP1\n","PEP\n","BABA1\n","NKE1\n","DIS1\n","NFLX1\n","EROS1\n","EROS\n","WWE1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zjRpQ7Zz8_is","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592207357084,"user_tz":-330,"elapsed":1072,"user":{"displayName":"Kushagra Mullick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOiLPke6TGBbvdbg4DS5sHovSbEc46n9iaox0QrQ=s64","userId":"05411353481505373953"}},"outputId":"3c470c46-5bd9-429c-9832-ada3b68b0f5d"},"source":["df_mas"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['MSFT',\n"," 'AAPL',\n"," 'AMZN',\n"," 'GOOGL',\n"," 'ORCL',\n"," 'JPM',\n"," 'BRK',\n"," 'GS',\n"," 'AXP',\n"," 'HSBC',\n"," 'V',\n"," 'BAC',\n"," 'T',\n"," 'JNJ',\n"," 'NVS',\n"," 'MRK',\n"," 'UNH',\n"," 'XOM',\n"," 'CVX',\n"," 'TOT',\n"," 'TM',\n"," 'F',\n"," 'NSANY',\n"," 'TSLA',\n"," 'GM',\n"," 'HOG',\n"," 'BA',\n"," 'DAL',\n"," 'LUV',\n"," 'UAL',\n"," 'AAL',\n"," 'SAVE',\n"," 'MCD',\n"," 'CMG',\n"," 'KMX',\n"," 'MUSA',\n"," 'CTB',\n"," 'SAH',\n"," 'GPC',\n"," 'H',\n"," 'LVS',\n"," 'MAR',\n"," 'PFE',\n"," 'PG',\n"," 'CHL',\n"," 'ABT',\n"," 'MDT',\n"," 'WMT',\n"," 'KO',\n"," 'PEP',\n"," 'BABA',\n"," 'NKE',\n"," 'DIS',\n"," 'NFLX',\n"," 'EROS',\n"," 'WWE',\n"," 'CNK',\n"," 'DELL',\n"," 'IBM',\n"," 'HPQ',\n"," 'FIT',\n"," 'HMI',\n"," 'TSM',\n"," 'USEG',\n"," 'PTR',\n"," 'SNP',\n"," 'UNP',\n"," 'SPCE',\n"," 'LMT',\n"," 'NOC',\n"," 'UPS',\n"," 'FDX',\n"," 'RIO',\n"," 'VZ',\n"," 'SAP',\n"," 'UL']"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"j_7Z3poWxfWq","colab_type":"code","colab":{}},"source":["dict34={'Stock1':stocks_1 , 'Stock2':stocks_2 , 'Min. pval':min_pvalue , 'Best lag':best_lag}\n","intraday_stock_granger=pd.DataFrame(data=dict34)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjHgu4Ptp8Dj","colab_type":"code","colab":{}},"source":["intraday_stock_granger"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ziT5fPstgXG1","colab_type":"code","colab":{}},"source":["consumer_key= 'BoSNF6BKUrGxY5XgOTvnOIXaz'\n","consumer_secret= 'VkNsq3jBtMHZYtlYEUZTT377yM1dHahzPNVbp8fmDDxDzEwE8C'\n","access_token= '1263072369280577536-Oqw2FpEPSXmwDffAGofAwV91L1A7Tz'\n","access_token_secret= 'H7qfOjemcbuusp1pO7p3oiSY026QvrgI4OA9KDo60dtb6'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWzsCE2cgbYm","colab_type":"code","colab":{}},"source":["class TwitterClient(object): \n","    def __init__(self): \n","        #Initialization method. \n","        try: \n","            # create OAuthHandler object \n","            auth = OAuthHandler(consumer_key, consumer_secret) \n","            # set access token and secret \n","            auth.set_access_token(access_token, access_token_secret) \n","            # create tweepy API object to fetch tweets \n","            # add hyper parameter 'proxy' if executing from behind proxy \"proxy='http://172.22.218.218:8085'\"\n","            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n","            \n","        except tweepy.TweepError as e:\n","            print(f\"Error: Tweeter Authentication Failed - \\n{str(e)}\")\n","\n","    def get_tweets(self, query, maxTweets = 1000):\n","        #Function to fetch tweets. \n","        # empty list to store parsed tweets \n","        tweets = [] \n","        sinceId = None\n","        max_id = -1\n","        tweetCount = 0\n","        tweetsPerQry = 100\n","\n","        while tweetCount < maxTweets:\n","            try:\n","                if (max_id <= 0):\n","                    if (not sinceId):\n","                        new_tweets = self.api.search(q=query, count=tweetsPerQry)\n","                    else:\n","                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n","                                                since_id=sinceId)\n","                else:\n","                    if (not sinceId):\n","                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n","                                                max_id=str(max_id - 1))\n","                    else:\n","                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n","                                                max_id=str(max_id - 1),\n","                                                since_id=sinceId)\n","                if not new_tweets:\n","                    print(\"No more tweets found\")\n","                    break\n","\n","                time.sleep(0.1)\n","\n","                for tweet in new_tweets:\n","                    parsed_tweet = {} \n","                    parsed_tweet['tweets'] = tweet.text \n","                    parsed_tweet['Date'] = tweet.created_at\n","\n","                    # appending parsed tweet to tweets list \n","                    if tweet.retweet_count > 0: \n","                        # if tweet has retweets, ensure that it is appended only once \n","                        if parsed_tweet not in tweets: \n","                            tweets.append(parsed_tweet) \n","                    else: \n","                        tweets.append(parsed_tweet) \n","                        \n","                tweetCount += len(new_tweets)\n","                print(\"Downloaded {0} tweets\".format(tweetCount))\n","                max_id = new_tweets[-1].id\n","\n","            except tweepy.TweepError as e:\n","                # Just exit if any error\n","                print(\"Tweepy error : \" + str(e))\n","                break\n","        \n","        return pd.DataFrame(tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jenBylC9gk6Y","colab_type":"code","colab":{}},"source":["twitter_client = TwitterClient()\n","\n","# calling function to get tweets\n","tweets_df = twitter_client.get_tweets('Apple', maxTweets=10000)\n","print(f'tweets_df Shape - {tweets_df.shape}')\n","tweets_df.tail(10)"],"execution_count":null,"outputs":[]}]}